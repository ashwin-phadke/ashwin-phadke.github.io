<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ashwin Phadke</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Ashwin Phadke</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© `2020` Ashwin Phadke for Hugo </copyright><lastBuildDate>Thu, 20 Aug 2020 21:38:51 +0530</lastBuildDate>
    <image>
      <url>/img/avatar</url>
      <title>Ashwin Phadke</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Load Tensorflow Models Using OpenCV</title>
      <link>/post/load-tensorflow-models-using-opencv/</link>
      <pubDate>Thu, 20 Aug 2020 21:38:51 +0530</pubDate>
      <guid>/post/load-tensorflow-models-using-opencv/</guid>
      <description>&lt;h3 id=&#34;background-&#34;&gt;Background :&lt;/h3&gt;
&lt;p&gt;It is always a daunting task with &lt;code&gt;Tensorflow&lt;/code&gt; sessions and standard handling of a typical &lt;code&gt;Tensorflow&lt;/code&gt; model when you want to run inference. However, if you are an experienced developer you may also quickly go through these steps because you are already aware about how to use &lt;code&gt;Tensorflow&lt;/code&gt; to run inference on your model.
Most of the times we use some image pre-processing over the input image before passing it to your model built using &lt;code&gt;Tensorflow&lt;/code&gt;. This pre-processing is mostly handled using OpenCV or such libraries or something like &lt;code&gt;imutils&lt;/code&gt; for basic handling of images or video frames.
How about using OpenCV itself to load and run inference on your &lt;code&gt;Tensorflow&lt;/code&gt; models, this is what I encountered very recently when I was stuck with a problem while using &lt;code&gt;Tensorflow&lt;/code&gt; sessions, although being straightforward these &lt;code&gt;Tensorflow&lt;/code&gt; sessions can sometimes take some little extra time to manage while doing inference. OpenCV handles it quite well and we are going to discuss it here in this blog post.&lt;/p&gt;
&lt;h3 id=&#34;pre-requisites-&#34;&gt;Pre-requisites :&lt;/h3&gt;
&lt;p&gt;You have been from&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import cv2
import tensorflow as tf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;results : person - 84%
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loading-tf-models-using-opencv-&#34;&gt;Loading TF models using OpenCV :&lt;/h3&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://docs.opencv.org/master/d2/d58/tutorial_table_of_content_dnn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Opencv&amp;rsquo;s DNN module&lt;/a&gt; hosts a variety of great features when it comes to utilizing the library for neural networks.&lt;/p&gt;
&lt;p&gt;One such important addition is 
&lt;a href=&#34;https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tensorflow&amp;rsquo;s object detection API&lt;/a&gt; using OpenCV&amp;rsquo;s dnn module.&lt;/p&gt;
&lt;p&gt;If you have not installed OpenCV with it&amp;rsquo;s extra modules also called as 
&lt;a href=&#34;https://github.com/opencv/opencv_contrib&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenCV Contrib&lt;/a&gt; modules you can read one of my previous post on how to do that 
&lt;a href=&#34;https://ashwin-phadke.github.io/post/install-opencv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;getting-started-&#34;&gt;Getting started :&lt;/h3&gt;
&lt;p&gt;Jump right to the end if you are looking to download the codes.
For this tutorial we require the Tensorflow models in a specific manner, needn&amp;rsquo;t worry because they are quite easily available through the TF model zoo.&lt;/p&gt;
&lt;p&gt;The result of training a model using Tensorflow is a binary file with extension .pb which contains both topology and weights of trained network. You may download one of them from 
&lt;a href=&#34;https://github.com/tensorflow/models&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Model Zoo&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;Once you have the &lt;code&gt;.pb&lt;/code&gt; file you will also need a &lt;code&gt;.pbtxt&lt;/code&gt; file which is an extra configuration file required which you can find 
&lt;a href=&#34;https://github.com/opencv/opencv_extra/tree/master/testdata/dnn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for many pretrained models and below you can find a good list from the OpenCV wiki to help you get started faster.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model Version&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Weights(.pb)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;prototxt(.pbtxt)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MobileNet-SSD v1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2017_11_17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/ssd_mobilenet_v1_coco_2017_11_17.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MobileNet-SSD v1 PPN&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2018_07_03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/ssd_mobilenet_v1_ppn_coco.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MobileNet-SSD v2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2018_03_29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/ssd_mobilenet_v2_coco_2018_03_29.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Inception-SSD v2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2017_11_17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/ssd_inception_v2_coco_2017_11_17.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MobileNet-SSD v3 (see 
&lt;a href=&#34;https://github.com/opencv/opencv/pull/16760&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#16760&lt;/a&gt;)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2020_01_14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://gist.github.com/dkurt/54a8e8b51beb3bd3f770b79e56927bd7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster-RCNN Inception v2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2018_01_28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/faster_rcnn_inception_v2_coco_2018_01_28.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster-RCNN ResNet-50&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2018_01_28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2018_01_28.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/faster_rcnn_resnet50_coco_2018_01_28.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mask-RCNN Inception v2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2018_01_28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/mask_rcnn_inception_v2_coco_2018_01_28.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EfficientDet-D0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(see 
&lt;a href=&#34;https://github.com/opencv/opencv/pull/17384&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#17384&lt;/a&gt;)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://www.dropbox.com/s/9mqp99fd2tpuqn6/efficientdet-d0.pb?dl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weights&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
&lt;a href=&#34;https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/efficientdet-d0.pbtxt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;config&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Furthermore, if you would like to convert your own models you can refer the following scripts for a better context to how to get the required files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/opencv/opencv/blob/master/samples/dnn/tf_text_graph_ssd.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf_text_graph_ssd.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/opencv/opencv/blob/master/samples/dnn/tf_text_graph_common.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf_text_graph_common.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/opencv/opencv/blob/master/samples/dnn/tf_text_graph_faster_rcnn.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf_text_graph_faster_rcnn.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/opencv/opencv/blob/master/samples/dnn/tf_text_graph_mask_rcnn.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tf_text_graph_mask_rcnn.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can pass the configuration file which was used for training to get your pbtxt file to determine the hyperparameters.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python tf_text_graph_faster_rcnn.py --input /path/to/model.pb --config /path/to/example.config --output /path/to/graph.pbtxt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the required files , let us dive right into the code.&lt;/p&gt;
&lt;p&gt;THe module we need is the &lt;code&gt;cv2&lt;/code&gt;&#39;s dnn module &lt;code&gt;readNetFromTensorflow&lt;/code&gt; which accepts &lt;code&gt;.pb&lt;/code&gt; and &lt;code&gt;.pbtxt&lt;/code&gt; as arguments.
According to opencv docs these arguments are defined as :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model - path to the .pb file with binary protobuf description of the network architecture
config - path to the .pbtxt file that contains text graph definition in protobuf format. Resulting Net object is built by text graph using weights from a binary one that let us make it more flexible. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To then load your model you can do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Net = cv2.dnn.readNetFromTensorflow(PATH_TO_CKPT, PATH_TO_PBTXT)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you have laoded your model with config you then need to pass your image/frame to the net to perform inference.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Net.setInput(cv2.dnn.blobFromImage(img, size=(300, 300), swapRB=True, crop=False))
detections = Net.forward()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;blobFromImage&lt;/code&gt; creates 4-dimensional blob from image. Optionally it also resizes and crops image from center, subtract mean values, scales values by scalefactor, swap Blue and Red channels.&lt;/p&gt;
&lt;p&gt;The parameters can be defined as shown in the above example&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;image - input image (with 1-, 3- or 4-channels).
size - spatial size for output image
mean - scalar with mean values which are subtracted from channels. Values are intended to be in (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.
scalefactor - multiplier for image values.
swapRB - flag which indicates that swap first and last channels in 3-channel image is necessary.
crop - flag which indicates whether image will be cropped after resize or not
ddepth - Depth of output blob. Choose CV_32F or CV_8U.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whereas the &lt;code&gt;Net.forward&lt;/code&gt; runs forward pass to compute output of layer with name outputName.
That&amp;rsquo;s it those are the changes, now you just need to loop over the detections with a desired accuracy metric and get your final result as you would otherwise get by using Tensorflow directly( bounding boxes ).&lt;/p&gt;
&lt;p&gt;You now need to put all the pieces together to getting started on this:&lt;/p&gt;
&lt;p&gt;To run inference on an image you can use the following script :&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/ashwin-phadke/22f76dfa33c2f13b67e28af559613c8e.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Well you are now set.&lt;/p&gt;
&lt;p&gt;You can expect output similar to this :&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ashwin-phadke/implementations-and-guides/master/wiki_contents/Screenshot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s just not stop right here, how about processing videos for a bit more addition to this along with the showing labels over those bounding boxes.
This post has you covered, you can use the following code to achieve this:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/ashwin-phadke/ea2b8739a173b4b2908c0886db69d308.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;You can also run your model using also the python script from the OpenCV documentation as given below:&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/ashwin-phadke/e0fe135edf2bcf62fb4e2f4c8905482b.js&#34;&gt;&lt;/script&gt;

&lt;h3 id=&#34;in-a-jiffy-&#34;&gt;In a jiffy :&lt;/h3&gt;
&lt;p&gt;We dived into loading tensorflow models using OpenCV, various definitions of the functions used for doing the &lt;code&gt;Tensorflow&lt;/code&gt; operations in OpenCV itself and some examples related to it.&lt;/p&gt;
&lt;p&gt;I believe using &lt;code&gt;Tensorflow&lt;/code&gt; directly is still also the most effective way you can use the trained or pre-trained models to run inference on , however for ease, simplicity you can also try this method as it also leverages the same &lt;code&gt;Tensorflow Object Detection&lt;/code&gt; API.
Hoping this was a great learning curve as it was for me.&lt;/p&gt;
&lt;p&gt;This can also be used to create API&amp;rsquo;s to your Opencv and Tensorflow based application.&lt;/p&gt;
&lt;p&gt;OpenCV can also be used with other popular deep learning frameworks and libraries like Torch, Caffe, ONNX and also supports many deep learning layers used for training your own neural networks alongwith also seamlessely integrating them.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/opencvtftutorial.zip&#34; class=&#34;buttonDownload&#34;&gt;Download codes for this tutorial&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;
body {
	background-color: #1a1a1a;
}
.buttonDownload{
	display: inline-block;
	position: relative;
	padding: 10px 25px;
  	background-color: #4CC713;
	color: white;
  	font-family: sans-serif;
	text-decoration: none;
	font-size: 0.9em;
	text-align: center;
	text-indent: 15px;
}
.buttonDownload:hover {
	background-color: #333;
	color: white;
}
.buttonDownload:before, .buttonDownload:after {
	content: &#39; &#39;;
	display: block;
	position: absolute;
	left: 15px;
	top: 52%;
}
/* Download box shape  */
.buttonDownload:before {
	width: 10px;
	height: 2px;
	border-style: solid;
	border-width: 0 2px 2px;
}
/* Download arrow shape */
.buttonDownload:after {
	width: 0;
	height: 0;
	margin-left: 3px;
	margin-top: -7px;
	border-style: solid;
	border-width: auto;
	border-color: transparent;
	border-top-color: inherit;
	animation: downloadArrow 2s linear infinite;
	animation-play-state: paused;
}
.buttonDownload:hover:before {
	border-color: #4CC713;
}
.buttonDownload:hover:after {
	border-top-color: #4CC713;
	animation-play-state: running;
}
/* keyframes for the download icon anim */
@keyframes downloadArrow {
	/* 0% and 0.001% keyframes used as a hackish way of having the button frozen on a nice looking frame by default */
	0% {
		margin-top: -7px;
		opacity: 1;
	}
	0.001% {
		margin-top: -15px;
		opacity: 0;
	}
	50% {
		opacity: 1;
	}
	100% {
		margin-top: 0;
		opacity: 0;
	}
}
&lt;/style&gt;
&lt;h3 id=&#34;references-&#34;&gt;References :&lt;/h3&gt;
&lt;p&gt;[1] OpenCV extra modules contrib - &lt;a href=&#34;https://github.com/opencv/opencv_extra/tree/master/testdata/dnn&#34;&gt;https://github.com/opencv/opencv_extra/tree/master/testdata/dnn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Tensorflow : &lt;a href=&#34;https://tensorflow.org&#34;&gt;https://tensorflow.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Stack overflow : &lt;a href=&#34;https://stackoverflow.com&#34;&gt;https://stackoverflow.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Tensorflow object detection API using OpenCV : &lt;a href=&#34;https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API&#34;&gt;https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Jean&amp;rsquo;s blog - &lt;a href=&#34;https://jeanvitor.com/tensorflow-object-detecion-opencv/&#34;&gt;https://jeanvitor.com/tensorflow-object-detecion-opencv/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] Deep learning using OpenCV : &lt;a href=&#34;https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV&#34;&gt;https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] COCO dataset - &lt;a href=&#34;http://cocodataset.org/#home&#34;&gt;http://cocodataset.org/#home&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] Google Protobuf - &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;https://developers.google.com/protocol-buffers/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] OpenCV Mask R-CNN sample - &lt;a href=&#34;https://github.com/opencv/opencv/blob/master/samples/dnn/mask_rcnn.py&#34;&gt;https://github.com/opencv/opencv/blob/master/samples/dnn/mask_rcnn.py&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Keras Tuner With Tensorflow</title>
      <link>/talk/keras-tuner-with-tensorflow/</link>
      <pubDate>Sat, 18 Jul 2020 11:00:00 +0530</pubDate>
      <guid>/talk/keras-tuner-with-tensorflow/</guid>
      <description>&lt;p&gt;Dive into an overview of hyperparameter tuning ro find the best hyperparameters for your deep learning model. The keras-tuner package can be used to define a search space for multiple hyperparameters like learning rate, optimizers, units among many others to facilite optimal hyperparameter search to obtain the best model for your desired datatset.&lt;/p&gt;
&lt;p&gt;Summary of the talk :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brief about convolutions, pooling.&lt;/li&gt;
&lt;li&gt;Introduction to Keras Tuner.&lt;/li&gt;
&lt;li&gt;Tunable hyperparameters.&lt;/li&gt;
&lt;li&gt;Overview of built in algorithms for hyperparameter tuning like Hyperband.&lt;/li&gt;
&lt;li&gt;Code  : Google colab demo&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Build your own layers for deep learning models using TensorFlow 2.0 and Python</title>
      <link>/post/build-layers-tf-python/</link>
      <pubDate>Tue, 23 Jun 2020 21:57:14 +0530</pubDate>
      <guid>/post/build-layers-tf-python/</guid>
      <description>&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;During a very recent 
&lt;a href=&#34;https://ashwin-phadke.github.io/talk/intro-to-deep-learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;webinar&lt;/a&gt; where I was a speaker for the topic &lt;code&gt;Deep learning with TensorFlow&lt;/code&gt; I repeatedly was asked a question regarding how would one really define their own layers, parameter and how they work so as to watch it do the magic while showing them some notebooks that had parameters to the layers that we regularly use. This prompted me to write a blog explaining layers and their parameters.&lt;/p&gt;
&lt;h3 id=&#34;building-tensorflow-layers&#34;&gt;Building Tensorflow Layers&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;tf.keras.layers.Layer&lt;/code&gt; or also written as &lt;code&gt;tf.compat.v1.keras.layers.Layer&lt;/code&gt; gives you easy and effective access to start writing your own layers in building the desired convolutional neural network.
Keras backend is well integrated with TensorFlow giving you ease of coding your layers at a high level understanding and handles a ot of code that you would otherwise would&amp;rsquo;ve written.&lt;/p&gt;
&lt;p&gt;Writing your own layers is not that daunting if you have a certain understanding on how they work.
According to the TensorFlow documentation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Many machine learning models are expressible as the composition and stacking of relatively simple layers, and TensorFlow provides both a set of many common layers as a well as easy ways for you to write your own application-specific layers either from scratch or as the composition of existing layers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which is pretty much True in the sense that these layers can be written like calling functions with arguments. Here in the &lt;code&gt;tf.keras.layers&lt;/code&gt; package the &lt;code&gt;layers&lt;/code&gt; that we want to define are treated as objects. So to construct a simple layer by yourself all you need is to construct the layer object and you are pretty much good to go.&lt;/p&gt;
&lt;p&gt;Some widely used layers include&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Conv1D&lt;/li&gt;
&lt;li&gt;Conv2D&lt;/li&gt;
&lt;li&gt;AvgPool1D&lt;/li&gt;
&lt;li&gt;Dense&lt;/li&gt;
&lt;li&gt;Flatten&lt;/li&gt;
&lt;li&gt;LSTMCell&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and many 
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/layers#classes_2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more&lt;/a&gt;.
To start defining your own layers is no hidden secret and can be easily achieved by doing (we will be looking at Sequential layers here)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = tf.keras.models.Sequntial()

or


model = tf.compat.v1.keras.Sequential()

or


model = tf.compat.v1.keras.models.Sequential()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the most important task of defining the type of your layers is done.
Let&amp;rsquo;s look at some of these layers and their functions one by one.
Once you are ready with your network or at least have an idea of how many layers you would like to write for a standard result you can proceed to now defining them. What Sequential essentially does it groups a linear stack of layers.
Once you have done that now you are ready to construct your own layer objects.&lt;/p&gt;
&lt;p&gt;After declaring above you can either do&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = tf.keras.models.Sequential()

model.add(tf.keras.layers.Dense(8, input_dim=16))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OR&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = tf.keras.Sequentail([ tf.keras.layers.Dense(8, input_dim=18)])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both these methods will help you achieve your goal of writing your own layers. To begin with let&amp;rsquo;s build our own complete network and understand each parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = tf.keras.models.Sequential([
  
  tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)),

  # Max pooling as we will take maximum value which is a 2X2 poll so every 4 pixels go to 1
  tf.keras.layers.MaxPooling2D(2, 2),

  # Another layer
  tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),
  tf.keras.layers.MaxPooling2D(2,2),

  # Converts the input to 1D set instead of the square we saw earlier
  tf.keras.layers.Flatten(),

  # Adds a layer of neurons
  tf.keras.layers.Dense(128, activation=&#39;relu&#39;),

  # The last layers have specific number of neurons, ask me why! :)

  tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)
])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now once we have built our layer, let&amp;rsquo;s try to understand it one by one.
We have already seen in brief about Sequential(), let&amp;rsquo;s now dive into the layers. The module &lt;code&gt;tf.keras.layers.Layer&lt;/code&gt; has different classes defined for all different kinds of models some of which are also listed above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the first layer &lt;code&gt;Conv2D(64, (3,3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)),&lt;/code&gt; refers to class 
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conv2D&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This layers creates a conv kernel to convolve with input to produce tensors.&lt;/li&gt;
&lt;li&gt;Activation function needs to be defined here and also if this layer is being used as the first layer in this model as shown in this example it is important to include &lt;code&gt;input_shape&lt;/code&gt; as in &lt;code&gt;input_shape = (128, 128, 3)&lt;/code&gt; for an image that is 128 X 128 pixel wide with three channels mainly RGB or use 1if it is a grey scale image.&lt;/li&gt;
&lt;li&gt;Arguments here include :
&lt;ul&gt;
&lt;li&gt;the filter size as 64 , which creates 64 filters to convolve over the input.&lt;/li&gt;
&lt;li&gt;the size of the kernel which is defined as (3, 3).&lt;/li&gt;
&lt;li&gt;activation is set to as &lt;code&gt;relu&lt;/code&gt;, what ReLU activation in brief means that the output values from this activation function are positives as all values in the left of the number line are counted as &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;the input_shape as defined above here is an grey scale image with size 28 X 28 pixels hence (28, 28, 1). Input shape can be defined as &lt;code&gt;(batch_size, rows, cols, channels)&lt;/code&gt; to understand order and meaning of the parameters.&lt;/li&gt;
&lt;li&gt;other arguments which can be added are &lt;code&gt;strides&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;use_bias&lt;/code&gt; and many more based on the requirements.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second layer &lt;code&gt;tf.keras.layers.MaxPooling2D(2, 2),&lt;/code&gt; is a maxpooling layer for which more information can be found 
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This layers creates a max pooling operation for 2D spatial data.&lt;/li&gt;
&lt;li&gt;Arguments here include :
&lt;ul&gt;
&lt;li&gt;MaxPooling is used to down-sample the data or the input to it by taking the maximum value from the window defined by &lt;code&gt;pool_size&lt;/code&gt; which here is (2, 2).&lt;/li&gt;
&lt;li&gt;The window is shifted by strides which are values by which we define by how many pixels our window moves ahead through the input representations.&lt;/li&gt;
&lt;li&gt;If padding is defined as &lt;code&gt;same&lt;/code&gt; the output shape becomes &lt;code&gt;output shape = input_shape / strides&lt;/code&gt; and &lt;code&gt;output shape = (input_shape - pool_size + 1) / strides)&lt;/code&gt; when padding size is defined as &lt;code&gt;valid&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;This returns 4D tensor representing maximum pooled values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The third layer &lt;code&gt;tf.keras.layers.Flatten()&lt;/code&gt; which is a layer in between the convolutional layer and a fully connected layer.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As evident from the name it flattens the input into a 1D vector that can be fed to a fully connected classifier layer.&lt;/li&gt;
&lt;li&gt;The main argument here &lt;code&gt;data_format&lt;/code&gt; as when &lt;code&gt;inputs are shaped (batch,) without a channel dimension, then flattening adds an extra channel dimension and output shapes are (batch, 1)&lt;/code&gt; - TFDocs.
In this case the input is 28X28X3 with 64 filter which makes the output of the Flatten layer to &lt;code&gt;64X28X28 = 50176&lt;/code&gt; as a 1D vector instead of multi-dimension as in previous layers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The fourth layer &lt;code&gt;  tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)&lt;/code&gt; adds a single layer to your network which is densely or fully connected.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each neuron declared here is connected to or receives input from all neurons from previous layers hence the name as densely connected.&lt;/li&gt;
&lt;li&gt;Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). - TFDocs&lt;/li&gt;
&lt;li&gt;The final layer is the layer that can be described here as output layer. The number &lt;code&gt;10&lt;/code&gt; here is the number of classes that we are fed or the number of outputs that are to be observed.&lt;/li&gt;
&lt;li&gt;One needs to understand that the output is not however a one shot yes or no but a list of probabilities of all the classes wherein then the highest probability is the prediction done by the model.&lt;/li&gt;
&lt;li&gt;More importantly we are using the function &lt;code&gt;softmax&lt;/code&gt; here which in simple words mean that the highest probability will be treated as 1 and all others as 0. This might sound somewhat like maxpool but these are two different concepts. Softmax function can also be loosely defined as the function that converts logits to probabilities that sum to 1. In short if the result of  classes is like [0.1, 0.4, 0.5] then the Softmax will return [0, 0, 1] . Known use-cases of softmax regression are in discriminative models such as Cross-Entropy and Noise Contrastive Estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Writing your own layers becomes more important when you want to build a fully custom solution which otherwise cannot be achieved using transfer learning(although being an decent method if you want a prototype). This helps you understand the tool you are using here TensorFlow and also get an understanding of how SoTA neural netwrks work and what do those layers mean or how the authors reached to those specific parameter values. We have networks ranging from 20 layers to more than a 100 with huge complexity which we aim to ease here by understanding the design process.&lt;/p&gt;
&lt;p&gt;I would also suggest going through the Tensorflow documentation extensively as they are a offiial resource of class and functions available in the TensorFlow API with a very good description of every parameter in detail.&lt;/p&gt;
&lt;h3 id=&#34;in-a-jiffy&#34;&gt;In a jiffy&lt;/h3&gt;
&lt;p&gt;We took a look at a higher level understanding of various TensorFlow layers, what do they mean and what do their arguments mean in brief. You can start building your own model from scratch(well let&amp;rsquo;s say just calling classes and functions :) ) and test them out by tuning various parameters mentioned and from the function definitions too and maybe you&amp;rsquo;ll soon have a best performing model in your profile. Happy coding!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://colab.research.google.com/drive/11dXFxMvuSdqhc2a63tvzJBcZ2IB4ESVL#scrollTo=PqNIM4OAkAZ1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code for reference&lt;/a&gt; |

&lt;a href=&#34;https://github.com/ashwin-phadke&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; |

&lt;a href=&#34;https://ashwin-phadke.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Website&lt;/a&gt; |&lt;/p&gt;
&lt;p&gt;References :&lt;/p&gt;
&lt;p&gt;[1] Tensorflow documentation - &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf&#34;&gt;https://www.tensorflow.org/api_docs/python/tf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Softmax function simpplified - &lt;a href=&#34;https://towardsdatascience.com/softmax-function-simplified-714068bf8156&#34;&gt;https://towardsdatascience.com/softmax-function-simplified-714068bf8156&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Wikipedia&lt;/p&gt;
&lt;p&gt;[4] Coursera - Tensorflow lectures by Laurence Moroney&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>[Invited]Introduction to Deep Learning using Tensorflow</title>
      <link>/talk/intro-to-deep-learning/</link>
      <pubDate>Wed, 17 Jun 2020 18:00:00 +0530</pubDate>
      <guid>/talk/intro-to-deep-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Computer Vision Playground</title>
      <link>/project/cvplayground/</link>
      <pubDate>Sun, 14 Jun 2020 22:02:15 +0530</pubDate>
      <guid>/project/cvplayground/</guid>
      <description>&lt;p&gt;A computer vision playground to try and test end to end(test to deploy) computer vision pipeline.
We are looking for open source enthusiasts to help advance the project, to know more click on contribute.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>/project/object-detection/</link>
      <pubDate>Wed, 10 Jun 2020 21:43:12 +0530</pubDate>
      <guid>/project/object-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Smart Anti Breach</title>
      <link>/project/smart-anti-breach/</link>
      <pubDate>Wed, 10 Jun 2020 21:38:15 +0530</pubDate>
      <guid>/project/smart-anti-breach/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Android chat application : Zappchat (from Rivchat)</title>
      <link>/project/zappchat/</link>
      <pubDate>Wed, 10 Jun 2020 21:36:51 +0530</pubDate>
      <guid>/project/zappchat/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Face Recognition using Python and OpenCV Haarcascades</title>
      <link>/project/face-recognition/</link>
      <pubDate>Wed, 10 Jun 2020 20:45:03 +0530</pubDate>
      <guid>/project/face-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optical Flow using OpenCV and Python</title>
      <link>/project/optical-flow/</link>
      <pubDate>Tue, 09 Jun 2020 22:26:21 +0530</pubDate>
      <guid>/project/optical-flow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Canny Edge Detection using OpenCV and C&#43;&#43;</title>
      <link>/project/canny-edge-detection/</link>
      <pubDate>Tue, 09 Jun 2020 22:21:49 +0530</pubDate>
      <guid>/project/canny-edge-detection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Find Dominant Color from an image using OpenCV and C&#43;&#43;</title>
      <link>/project/find-dominant-color/</link>
      <pubDate>Tue, 09 Jun 2020 22:17:31 +0530</pubDate>
      <guid>/project/find-dominant-color/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Installing OpenCV with contrib extra modules from source on Fedora Linux</title>
      <link>/post/install-opencv/</link>
      <pubDate>Tue, 09 Jun 2020 20:45:46 +0530</pubDate>
      <guid>/post/install-opencv/</guid>
      <description>&lt;p&gt;Why the sudden need?
I have mostly used the OpenCV library as available from pip or by installing wheel until I required some more essential features that are not available from those pre built binaries. However the installation looking fairly simple can have a lot of issues if not done correctly and hence the reason for being so mnay questions on forums out there on the internet pertaining to the installation issues:&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin :&lt;/p&gt;
&lt;p&gt;Although it is always a best practice to create a virtual environment so as to keep your project dependencies sorted but skipping the step here. If you want to create one or know more about those you can visit 
&lt;a href=&#34;https://docs.python.org/3/tutorial/venv.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this link&lt;/a&gt; for more information .&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clone &lt;code&gt;git clone https://github.com/opencv/opencv.git&lt;/code&gt; the official 
&lt;a href=&#34;https://github.com/opencv/opencv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenCV repository&lt;/a&gt; from Github to have the latest updated version for use.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will create a folder named opencv in your home directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some dependencies need to be taken care of before building OpenCV for use. These dependencies are &lt;strong&gt;IMPORTANT&lt;/strong&gt; to facilitate the proper installation of opencv on the system and avoiding errors like &lt;code&gt;no module found&lt;/code&gt; and etc even after having a successful install.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The must have dependenices :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;         * dnf install cmake
         * dnf install python-devel numpy
         * dnf install gcc gcc-c++
         * dnf install make
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GTK support :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    * dnf install gtk2-devel
    * dnf install libdc1394-devel
    * dnf install libv4l-devel
    * dnf install ffmpeg-devel
    * dnf install gstreamer-plugins-base-devel
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For parallelizing functions in opencv on Intel machines you can install Threaded building blocks &lt;code&gt;yum install tbb-devel&lt;/code&gt; . If you install this pass -D WITH_TBB=ON while building OpenCV with CMake.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Similarly for optimized mathematical operations install &lt;code&gt;dnf install eigen3-devel&lt;/code&gt; and pass -D WITH_EIGEN=ON as earlier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a folder named build wherein you&amp;rsquo;ll configure and build the library to later install by doing
&lt;code&gt;mkdir build&lt;/code&gt;
&lt;code&gt;cd build&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Installing OpenCV - OpenCV is typically installed using CMake as(explanation from the documentation) - Installation has to be configured with CMake. It specifies which modules are to be installed, installation path, which additional libraries to be used, whether documentation and examples to be compiled etc. Below command is normally used for configuration (executed from build folder).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To configure build parameters using CMake you will pass the following command for minimal installation in release mode&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you have installed the threading building blocks then you pass&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. -D WITH_TBB=ON&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you have installed the eigen library then do&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. -D WITH_EIGEN=ON&lt;/code&gt;  .&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If installed both then&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. -D WITH_EIGEN=ON -D WITH_TBB=ON&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now to also include non free algorithms(optional) you will need to add the specific parameter to it as :&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. -D WITH_EIGEN=ON -D WITH_TBB=ON -D OPENCV_ENABLE_NONFREE=ON&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We have now set up our opencv installation command, many a times opencv&amp;rsquo;s extra modules also help you solve many computer vision challentasks and it is better to have the contrib i.e extra modules with opencv as a additional step. Hence to do so first clone the opencv contrib module from Github using&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git clone https://github.com/opencv/opencv_contrib.git&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;to a seperate folder than your opencv directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Doing so it is then required to pass the path to the downloaded contrib modules to the build configuration of opencv. To do that use the follwoing command&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. -D WITH_EIGEN=ON -D WITH_TBB=ON -D OPENCV_ENABLE_NONFREE=ON -D OPENCV_EXTRA_MODULES_PATH=&amp;lt;opencv_contrib_path&amp;gt;/modules &amp;lt;opencv_source_directory_path&amp;gt;&lt;/code&gt; .&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will serve as your final command to installl opencv with all the required modules.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finally execute &lt;code&gt;make install&lt;/code&gt; to successfully build and install the OpenCV library on your Linux Fedora machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Invited to talk about Understanding evaluation metrics</title>
      <link>/talk/pdctalk/</link>
      <pubDate>Mon, 08 Jun 2020 23:09:18 +0530</pubDate>
      <guid>/talk/pdctalk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Confusion matrix for machine learning in a simple way</title>
      <link>/post/ml-dl-confusion-matrix/</link>
      <pubDate>Wed, 20 May 2020 20:50:31 +0530</pubDate>
      <guid>/post/ml-dl-confusion-matrix/</guid>
      <description>&lt;p&gt;Machine learning basic concepts explained as notes from various courses, books and blogs to my understanding.&lt;/p&gt;
&lt;h1 id=&#34;confusion-matrix&#34;&gt;Confusion Matrix&lt;/h1&gt;
&lt;p&gt;Confusion matrix as a evaluation metric has been discussed a lot. Indeed, the reason being it&amp;rsquo;s wide usability and f1-score being a important metric in order to evaluate those classification models. Here we take a look at understanding the confusion matrix in a rather simple way.&lt;/p&gt;
&lt;p&gt;Let us assume a dataset with around 40 rows of customer chrun according to a survey from a leading company. The confusion matrix herein will include the matrix of corrected and wrong predictions done on actual labels i.e the outcomes of the preditions on actual labels.&lt;/p&gt;
&lt;p&gt;The matrix is represented as :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;rows :  Contain actual or true labels in the test set i.e here the labels given by the survey.

columns : Contain predicted labels by the classifier trained on the customer survey dataset.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is imperative to understand this relation so as to identify how accurate was the prediction done from the classifier.&lt;/p&gt;
&lt;p&gt;We are going to see a binary classifcation example.&lt;/p&gt;
&lt;p&gt;We know that such evalutaion metrics shows the models ability to correctly predict or seperate the classes.
We will be also discussing f1-score here which is also important to learn along with confusion matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/ashwin-phadke/implementations-and-guides/blob/master/wiki_contents/confmatrix.jpg&#34; alt=&#34;Matrix&#34; title=&#34;Matrix Image&#34;&gt;&lt;/p&gt;
&lt;p&gt;The confusion matrix helps you measure some of very important metrics to assess performance of your model like .&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- Precision.

- Recall.

- Accuracy and plot ROC-AUC curve too.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us first dive into ddefining certain terminologies to better understand the concept of confusion matrix.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- True Positive : Model predicted true and it is correct as per labels.

- False Positive : Model predicted true but it is actually false.

- True Negative : Model predicted false and it is false.

- False Negative : Model predicted false but it is True.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/ashwin-phadke/implementations-and-guides/blob/master/wiki_contents/abc.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt;
&lt;p&gt;The image clearly illustrates how these values are divided in the matrix.&lt;/p&gt;
&lt;p&gt;We will take a look at how to calculate the values for the metrics derived from this matrix(lol, rhyme)&lt;/p&gt;
&lt;p&gt;Precision is a measure of accuracy provided that a class label has been predicted.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Precision = True Positive 
            __________________________

            True Positve + False Positive
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Recall = True Positive 
        ______________________________

        True Positve + False Negative
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
f1-score = 2 X Precision + Recall
            ______________________

            Precision X Recall

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, average accuracy can be calculated using average of all f1-scores and the desired values is 1. F1-score can also be used for multi class classifiers.&lt;/p&gt;
&lt;p&gt;If you want to learn more you can always refer to 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt; for more.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Getting started with SQL server on Linux (using Azure data studio)</title>
      <link>/post/my-article-name/</link>
      <pubDate>Tue, 03 Mar 2020 20:42:55 +0530</pubDate>
      <guid>/post/my-article-name/</guid>
      <description>&lt;p&gt;Here is a guide to getting started with SQL Server and Azure Data Studio on Linux.&lt;/p&gt;
&lt;p&gt;When I was starting on in my career , databases always meant the MySQL and PHP way to implement anything. However even though MySQL is really a gem SQL Server and it&amp;rsquo;s functionality gives you the full enterprise like experience and ease to implement into your code and especially the activity manager aka activity monitor which I fell for immediately.&lt;/p&gt;
&lt;p&gt;Starting with SQL server and that too Linux is not very easy a task. Mostly the primary reason being that all the config files and supporting installation scripts are &lt;code&gt;python 2.x&lt;/code&gt; compatible which is strange considering how long ago &lt;code&gt;python 3.x&lt;/code&gt; was launched. I did not have &lt;code&gt;python 2.x&lt;/code&gt; and did not want two python versions on my PC so I took up the task to convert most of the available installation scripts to support &lt;code&gt;python 3.x&lt;/code&gt; and which is yet in progress giving the sheer amount of things needed to change.
Meanwhile the development couldn&amp;rsquo;t have waited and hence I am writing a guide to facilitate ease in installing SQL Server on Linux. The documentation is pretty good in describing things but also is a lot of different pages to visit for a simple task. Hence adding it all under one page here.&lt;/p&gt;
&lt;p&gt;My Machine configuration is as follows(for reference) :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intel I3-6006U&lt;/li&gt;
&lt;li&gt;4GB / 1TB&lt;/li&gt;
&lt;li&gt;Dual boot
&lt;ul&gt;
&lt;li&gt;Fedora 31&lt;/li&gt;
&lt;li&gt;Windows 10&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python 2.7 and 3.5 with pip package manager.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To successfully install SQL server and start using it follow the following steps carefully:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Update your system : Run &lt;code&gt;sudo dnf update&lt;/code&gt; followed with &lt;code&gt;sudo dnf upgrade&lt;/code&gt;
to have the most recent updates to your Fedora machine.
If you are using Debian/Ubuntu you can simply do &lt;code&gt;sudo apt-get update&lt;/code&gt; and &lt;code&gt;sudo apt-get upgrade&lt;/code&gt;            respectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download SQL configuration file from Red Hat repository :&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;sudo curl -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/8/mssql-server-2019.repo&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;To Install the SQL server run :&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;sudo yum install -y mssql-server&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This will install all the primary requirements for SQL server.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Now to configure your SQL server installation you need to run the following command in order to complete the installation. The configuration step includes various options to add/remove add-ons to your SQL server installation and also to choose your edition.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;sudo /opt/mssql/bin/mssql-conf setup&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;
&lt;p&gt;This will fully install SQL server on your machine. Installation doesn&amp;rsquo;t take much time as most of the stuff is already done in Step 4 and step 5 includes configurations and settings that are needed to be installed in order to run the server.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;systemctl status mssql-server&lt;/code&gt; to check whether the service is running or not. You can also use the standard &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;stop&lt;/code&gt; for &lt;code&gt;systemctl&lt;/code&gt; to change the service state.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So far we have installed SQL server but we also need to be able to access it, you can use &lt;code&gt;pyodbc&lt;/code&gt; for all your python needs , also UnixODBC to connect via python program and/or command line. For that you will need some command line tools which you can learn to install here following steps by clicking 
&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-red-hat?view=sql-server-ver15#tools&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; .
The reason for not including here is :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is already included on the link like the above steps too.&lt;/li&gt;
&lt;li&gt;The focus here is SQL Server + Azure Data Studio&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;
&lt;p&gt;To install Azure Data studio in a very simple way click 
&lt;a href=&#34;https://azuredatastudiobuilds.blob.core.windows.net/releases/1.15.1/azuredatastudio-linux-1.15.1.rpm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here for version 1.15.1&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After the successful installation it will show up in your app directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simply start using command &lt;code&gt;azuredatastudio&lt;/code&gt; in terminal or by clicking Azure data studio in apps and connect with the username and password that you had configured in step 4. Default username is &lt;code&gt;SA&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You have now successfully managed to install SQL Server using Azure Data Studio on Linux(Fedora).&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Compiling and executing OpenCV programs written in C&#43;&#43;</title>
      <link>/post/compile-cpp-program-opencv/</link>
      <pubDate>Thu, 09 Jan 2020 20:48:22 +0530</pubDate>
      <guid>/post/compile-cpp-program-opencv/</guid>
      <description>&lt;h1 id=&#34;compiling-and-executing-c-programs&#34;&gt;Compiling and executing C++ programs&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Browse to the directory of your cpp program and open a terminal in that folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a CMake file name CMakeLists.txt in the same directory as your project :&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;              cmake_minimum_required(VERSION 2.8)
              project( ProjectName )
              find_package( OpenCV REQUIRED )`
              add_executable( ProjectName your_program_main_file.cpp )
              target_link_libraries( ProjectName ${OpenCV_LIBS} ) 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now we can generate the executable so as to run the program.
To do so execute&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt; cmake .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;followed by&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; make &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;on the terminal. The execution will run successfully and create binaries if there are no errors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To execute your compiled program enter &lt;code&gt;./ProjectName&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
